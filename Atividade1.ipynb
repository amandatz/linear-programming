{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amandatz/linear-programming/blob/main/Atividade1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkGVnRe9CPdx"
      },
      "source": [
        "# Atividade 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thdM6TiJWo3c"
      },
      "source": [
        "Amanda Topanotti Zanette (22100776)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRXSFVsDWq4e"
      },
      "source": [
        "**Importações e funções auxiliares**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HWjqEDuVIgTF"
      },
      "outputs": [],
      "source": [
        "using LinearAlgebra, Printf, Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esse trecho de código é usado apenas para garantir a repetibilidade dos números aleatórios gerados."
      ],
      "metadata": {
        "id": "7xszQjRRAATk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using Random\n",
        "Random.seed!(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayAFiLALAAyM",
        "outputId": "42d657a6-c8b6-48e0-a4ec-fc6be132aeb2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskLocalRNG()"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC9tSXZLCTw9"
      },
      "source": [
        "## Questão 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradiente com backtracking**"
      ],
      "metadata": {
        "id": "gpNzvk14M1lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "function steepest_descent_backtracking(fun, x0; sigma=1e-4, rho=0.5, max_iter=2000, tol=1e-6)\n",
        "  x = copy(x0)\n",
        "\n",
        "  for k = 0:max_iter-1\n",
        "    f, g, _ = fun(x)\n",
        "    gnorm = norm(g)\n",
        "\n",
        "    if gnorm <= tol\n",
        "      return x, k, gnorm\n",
        "    end\n",
        "\n",
        "    # backtracking\n",
        "    t = 1.0\n",
        "    xn = x - t*g\n",
        "    fn, _, _= fun(xn)\n",
        "\n",
        "    while fn > f - sigma*t*gnorm^2\n",
        "      t *= rho\n",
        "      xn = x - t*g\n",
        "      fn, _, _ = fun(xn)\n",
        "    end\n",
        "\n",
        "    x = xn\n",
        "  end\n",
        "\n",
        "  _, g, _ = fun(x)\n",
        "  gnorm = norm(g)\n",
        "  return x, max_iter, gnorm\n",
        "end\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdjzmeIAGMf-",
        "outputId": "990a6261-eb6e-48c7-808d-f11a5d7d886a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "steepest_descent_backtracking (generic function with 1 method)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Newton com backtracking**"
      ],
      "metadata": {
        "id": "HndsAOTKM3zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método de Newton é dado por\n",
        "$$\n",
        "  x_{k+1} = x_k - \\alpha_k H(x_k)^{-1} \\nabla f(x_k)\n",
        "$$\n",
        "em que\n",
        "- $\\nabla f(x_k)$: gradiente de f;\n",
        "- $H(x_k)$: matriz hessiana de f;\n",
        "- $\\alpha_k$: tamanho do passo."
      ],
      "metadata": {
        "id": "lSkkMof1ov6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "function newton_backtracking(fun, x0; sigma=1e-4, rho=0.5, max_iter=2000, tol=1e-6, fallback=nothing)\n",
        "  x = copy(x0)\n",
        "\n",
        "  for k = 0:max_iter-1\n",
        "    f, g, H = fun(x)\n",
        "    gnorm = norm(g)\n",
        "\n",
        "    if gnorm <= tol\n",
        "      return x, k, gnorm\n",
        "    end\n",
        "\n",
        "    p = -(H \\ g)\n",
        "\n",
        "    # se não for direção de descida, usa fallback se existir\n",
        "    if fallback != nothing && g' * p >= 0\n",
        "      p = fallback(g, H)\n",
        "    end\n",
        "\n",
        "    # backtracking\n",
        "    t = 1.0\n",
        "    xn = x + t*p\n",
        "    fn, _, _ = fun(xn)\n",
        "\n",
        "    while fn > f + sigma*t*(g' * p)\n",
        "        t *= rho\n",
        "        xn = x + t*p\n",
        "        fn, _, _ = fun(xn)\n",
        "    end\n",
        "\n",
        "    x = xn\n",
        "  end\n",
        "\n",
        "  f, g, H = fun(x)\n",
        "  return x, max_iter, norm(g)\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ5v9Z5ONXUu",
        "outputId": "1c418538-62dd-4e15-d7da-a26b8fad0e29"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "newton_backtracking (generic function with 1 method)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnekAseLCVy8"
      },
      "source": [
        "## Questão 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considere a função de Rosenbrock"
      ],
      "metadata": {
        "id": "3xS6VTM-Bn0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "function rosenb(x)\n",
        "  a = 1.0\n",
        "  b = 10.0\n",
        "\n",
        "  f = (a - x[1])^2 + b*(x[2] - x[1]^2)^2\n",
        "\n",
        "  grad_f = [ -2*(a - x[1]) + 2*b*(x[2] - x[1]^2)*(-2*x[1]),\n",
        "        2*b*(x[2] - x[1]^2)]\n",
        "\n",
        "  hess_f = [\n",
        "        2 - 4*b*(x[2] - x[1]^2) + 8*b*x[1]^2   -4*b*x[1];\n",
        "        -4*b*x[1]                               2*b\n",
        "  ]\n",
        "\n",
        "  return f, grad_f, hess_f\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clYbT8rbBw1D",
        "outputId": "600de792-4eb8-47e6-fa54-28e7e1724b5b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rosenb (generic function with 1 method)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparemos ambos os métodos anteriores considerando os pontos iniciais $x_0 = (1.01, 1.01)$ e $x_0 = (-1.0, 1.2)$"
      ],
      "metadata": {
        "id": "Mx_AfghMCt4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = [1.01, 1.01]\n",
        "\n",
        "x, max_iter_sd, gnorm_sd = steepest_descent_backtracking(rosenb, x0)\n",
        "@printf(\"Gradiente:\\n  Número de iterações: %d\\n  Gradiente final: %.6e\\n  Ponto final: [%.6f, %.6f]\\n\\n\",\n",
        "        max_iter_sd, gnorm_sd, x[1], x[2])\n",
        "\n",
        "x, max_iter_n, gnorm_n = newton_backtracking(rosenb, x0)\n",
        "@printf(\"Newton:\\n  Número de iterações: %d\\n  Gradiente final (norma): %.6e\\n  Ponto final: [%.6f, %.6f]\\n\\n\",\n",
        "        max_iter_n, gnorm_n, x[1], x[2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em0_EC6vDKNG",
        "outputId": "83290a7b-05d6-45d4-bab7-5c697974d626"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradiente:\n",
            "  Número de iterações: 1027\n",
            "  Gradiente final: 9.818612e-07\n",
            "  Ponto final: [1.000001, 1.000002]\n",
            "\n",
            "Newton:\n",
            "  Número de iterações: 3\n",
            "  Gradiente final (norma): 4.895830e-10\n",
            "  Ponto final: [1.000000, 1.000000]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para $x_0 = (1.01, 1.01)$ próximo do mínimo $x^* = (1,1)$, o método de Newton converge quadraticamente, resultando em apenas 3 iterações, enquanto o método do gradiente precisou de 1027 iterações. Além disso, o método de Newton foi bem mais preciso quando comparado ao método do gradiente."
      ],
      "metadata": {
        "id": "BUSTmUFzEIbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = [-1.0, 1.2]\n",
        "\n",
        "x, max_iter_sd, gnorm_sd = steepest_descent_backtracking(rosenb, x1)\n",
        "@printf(\"Gradiente:\\n  Número de iterações: %d\\n  Gradiente final: %.6e\\n  Ponto final: [%.6f, %.6f]\\n\\n\",\n",
        "        max_iter_sd, gnorm_sd, x[1], x[2])\n",
        "\n",
        "x, max_iter_n, gnorm_n = newton_backtracking(rosenb, x1, max_iter=5000)\n",
        "@printf(\"Newton:\\n  Número de iterações: %d\\n  Gradiente final (norma): %.6e\\n  Ponto final: [%.6f, %.6f]\\n\\n\",\n",
        "        max_iter_n, gnorm_n, x[1], x[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ek9SUpKDS6I",
        "outputId": "3c161e9e-7c44-480d-9eca-0eb6e7a5de98"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradiente:\n",
            "  Número de iterações: 1184\n",
            "  Gradiente final: 9.714477e-07\n",
            "  Ponto final: [0.999999, 0.999998]\n",
            "\n",
            "Newton:\n",
            "  Número de iterações: 5000\n",
            "  Gradiente final (norma): 5.656854e+00\n",
            "  Ponto final: [-1.000000, 1.200000]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para $x_0 = (-1.0, 1.2)$, mais distante, Newton falha em convergir (5000 iterações) porque $H(x)$ longe do mínimo não é positiva definida, enquanto o gradiente ainda converge (1184 iterações).\n"
      ],
      "metadata": {
        "id": "_31aotrtRis2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe que, para a função de Rosenbrock\n",
        "\n",
        "$$\n",
        "  f(x_1, x_2) = (x_1 - 1)^2 + 10 (x_2 - x_1^2)^2,\n",
        "$$\n",
        "\n",
        "a Hessiana no ponto $x_0 = (-1, 1.2)$ é\n",
        "\n",
        "$$\n",
        "  H(x) =\n",
        "  \\begin{bmatrix}\n",
        "  74 & 40 \\\\\n",
        "  40 & 20\n",
        "  \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "e o determinante é\n",
        "\n",
        "$$\n",
        "  \\det(H) = -120 < 0\n",
        "$$\n",
        "\n",
        "ou seja, a Hessiana **não é positiva definida** nesse ponto. Isso explica por que o método de Newton não converge a partir desse ponto inicial.\n"
      ],
      "metadata": {
        "id": "nWhx8UtUEbbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ajustar o algoritmo de Newton de modo que, caso a Hessiana não seja positiva definida, o método utilize o gradiente como alternativa."
      ],
      "metadata": {
        "id": "wx89YnSjMK4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_fallback(g, H) = -g / norm(g)\n",
        "\n",
        "x, max_iter_n, gnorm_n = newton_backtracking(rosenb, x1, max_iter=5000, fallback=gradient_fallback)\n",
        "@printf(\"Newton:\\n  Número de iterações: %d\\n  Gradiente final (norma): %.6e\\n  Ponto final: [%.6f, %.6f]\\n\\n\",\n",
        "        max_iter_n, gnorm_n, x[1], x[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6xQOcQFLIAz",
        "outputId": "7aa5458c-f06e-490c-ca3b-b147ee5d388c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Newton:\n",
            "  Número de iterações: 12\n",
            "  Gradiente final (norma): 8.406475e-08\n",
            "  Ponto final: [1.000000, 1.000000]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzBgumq1CYA2"
      },
      "source": [
        "## Questão 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "function exact_line_search_quadratic(A, b, x, d)\n",
        "  numerator = dot(b, d) - dot(d, A * x)\n",
        "  denominator = dot(d, A * d)\n",
        "\n",
        "  if denominator <= 0 || abs(denominator) < 1e-14\n",
        "    return 1e-6   # passo mínimo seguro\n",
        "  end\n",
        "  return numerator / denominator\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAFJt8bFrv1U",
        "outputId": "db00e8d4-0d22-4798-cf08-726879b17364"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "exact_line_search_quadratic (generic function with 1 method)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "function quadratic(x, A, b)\n",
        "  f = 0.5 * dot(x, A * x) - dot(b, x)\n",
        "  g = A * x - b\n",
        "  H = A\n",
        "  return f, g, H\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryacwBarqdTh",
        "outputId": "3acaaac2-9c46-4df8-8abd-f12b285d85b7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "quadratic (generic function with 1 method)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "function householder_matrix(n)\n",
        "  u = randn(n)\n",
        "  u /= norm(u)\n",
        "  U = Matrix(I, n, n) - 2.0 * (u * u')\n",
        "  return U\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usNTyzyP0OEg",
        "outputId": "6ad3ca16-6b83-44a3-a770-0cb674022d12"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "householder_matrix (generic function with 1 method)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradiente com busca linear exata**"
      ],
      "metadata": {
        "id": "hkm5yTJVhOv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "function steepest_descent_exact(A, b, x0; max_iter=2000, tol=1e-6)\n",
        "  x = copy(x0)\n",
        "\n",
        "  for k = 0:max_iter-1\n",
        "    f, g, _ = quadratic(x, A, b)\n",
        "\n",
        "    gnorm = norm(g)\n",
        "    if gnorm <= tol\n",
        "      return x, k, gnorm\n",
        "    end\n",
        "\n",
        "    d = -g\n",
        "\n",
        "    # Busca linear exata\n",
        "    alpha = exact_line_search_quadratic(A, b, x, d)\n",
        "\n",
        "    x = x + alpha * d\n",
        "  end\n",
        "\n",
        "  _, g, _ = quadratic(x, A, b)\n",
        "  return x, max_iter, norm(g)\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSWeM9ScsFPN",
        "outputId": "472c4645-7204-4572-95ef-421fdb97a8fa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "steepest_descent_exact (generic function with 1 method)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Newton com busca linear exata**"
      ],
      "metadata": {
        "id": "vV_BWIVFhRvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "function newton_exact(A, b, x0; max_iter=2000, tol=1e-6)\n",
        "  x = copy(x0)\n",
        "\n",
        "  for k = 0:max_iter-1\n",
        "    f, g, H = quadratic(x, A, b)\n",
        "\n",
        "    gnorm = norm(g)\n",
        "    if gnorm <= tol\n",
        "      return x, k, gnorm\n",
        "    end\n",
        "\n",
        "    p = - H \\ g\n",
        "\n",
        "    # passo exato\n",
        "    alpha = exact_line_search_quadratic(A, b, x, p)\n",
        "\n",
        "    x = x + alpha * p\n",
        "  end\n",
        "\n",
        "  _, g, _ = quadratic(x, A, b)\n",
        "  return x, max_iter, norm(g)\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLL9JItHMYah",
        "outputId": "aeeb4eca-502a-4043-dfda-d0113a5d958e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "newton_exact (generic function with 1 method)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O próximo código cria uma instância quadrática $f(x) = \\tfrac{1}{2} x^T A x - b^T x$ de dimensão $n$ com as distribuições de autovalores:  \n",
        "- `:equal`: todos os autovalores iguais;\n",
        "- `:two`: apenas dois valores diferentes;\n",
        "- `:spread`: autovalores variados de 1 até 1000."
      ],
      "metadata": {
        "id": "s1Xx8LPVhsb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "function generate_quadratic_instance(n, case)\n",
        "  U = householder_matrix(n)\n",
        "\n",
        "  eigenvalues = zeros(n)\n",
        "\n",
        "  if case == :equal\n",
        "    eigenvalues .= 1.0\n",
        "  elseif case == :two\n",
        "    eigenvalues[1:div(n,2)] .= 2.0\n",
        "    eigenvalues[div(n,2)+1:end] .= 5.0\n",
        "    shuffle!(eigenvalues)\n",
        "  elseif case == :spread\n",
        "    eigenvalues .= collect(LinRange(1.0, 1000.0, n))\n",
        "    shuffle!(eigenvalues)\n",
        "  end\n",
        "\n",
        "  Lambda = Diagonal(eigenvalues)\n",
        "  A = U * Lambda * U'\n",
        "  A = 0.5 * (A + A')   # garante simetria\n",
        "\n",
        "  b = randn(n)\n",
        "  x0 = randn(n)\n",
        "\n",
        "  return A, b, x0, eigenvalues\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6aRxP-0yTMk",
        "outputId": "9a133107-ee7c-4bc6-c8d4-4952377514ad"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "generate_quadratic_instance (generic function with 1 method)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste bloco, aplicamos os métodos de otimização (`steepest_descent_exact` e `newton_exact`) com diferentes dimensões e distribuições de autovalores."
      ],
      "metadata": {
        "id": "yYN8ENPaiIut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dimensions = [10, 100, 1000]\n",
        "cases = [:equal, :two, :spread]\n",
        "\n",
        "for n in dimensions, case in cases\n",
        "  A, b, x0, eigenvalues = generate_quadratic_instance(n, case)\n",
        "\n",
        "  # Gradiente\n",
        "  t_sd = @elapsed x_sd, k_sd, gnorm_sd = steepest_descent_exact(A, b, x0, max_iter=10000)\n",
        "\n",
        "  # Newton\n",
        "  t_newt = @elapsed x_newt, k_newt, gnorm_newt = newton_exact(A, b, x0, max_iter=10000)\n",
        "\n",
        "  println(\"Dimensão: $n, Caso: $case\")\n",
        "  println(\"  Gradiente:\")\n",
        "  println(\"    Iterações: $k_sd\")\n",
        "  println(\"    Tempo: $(round(t_sd, digits=6)) s\")\n",
        "  println(\"    Gradiente: $gnorm_sd\")\n",
        "  println(\"  Newton:\")\n",
        "  println(\"    Iterações: $k_newt\")\n",
        "  println(\"    Tempo: $(round(t_newt, digits=6)) s\")\n",
        "  println(\"    Gradiente: $gnorm_newt \\n\")\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CYt3iuHMbHo",
        "outputId": "6ed035f9-5145-401b-c48d-343dfbcb020d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensão: 10, Caso: equal\n",
            "  Gradiente:\n",
            "    Iterações: 1\n",
            "    Tempo: 2.4e-5 s\n",
            "    Gradiente: 4.0199198921899116e-16\n",
            "  Newton:\n",
            "    Iterações: 1\n",
            "    Tempo: 2.3e-5 s\n",
            "    Gradiente: 2.6588280504982636e-16 \n",
            "\n",
            "Dimensão: 10, Caso: two\n",
            "  Gradiente:\n",
            "    Iterações: 19\n",
            "    Tempo: 3.8e-5 s\n",
            "    Gradiente: 4.5377253912453864e-7\n",
            "  Newton:\n",
            "    Iterações: 1\n",
            "    Tempo: 1.3e-5 s\n",
            "    Gradiente: 2.046397339490942e-15 \n",
            "\n",
            "Dimensão: 10, Caso: spread\n",
            "  Gradiente:\n",
            "    Iterações: 6799\n",
            "    Tempo: 0.017562 s\n",
            "    Gradiente: 9.987694876074239e-7\n",
            "  Newton:\n",
            "    Iterações: 1\n",
            "    Tempo: 2.4e-5 s\n",
            "    Gradiente: 2.7753807806720287e-13 \n",
            "\n",
            "Dimensão: 100, Caso: equal\n",
            "  Gradiente:\n",
            "    Iterações: 1\n",
            "    Tempo: 4.6e-5 s\n",
            "    Gradiente: 4.852626736724029e-15\n",
            "  Newton:\n",
            "    Iterações: 1\n",
            "    Tempo: 0.000245 s\n",
            "    Gradiente: 3.768423396013266e-15 \n",
            "\n",
            "Dimensão: 100, Caso: two\n",
            "  Gradiente:\n",
            "    Iterações: 17\n",
            "    Tempo: 0.000217 s\n",
            "    Gradiente: 3.439588506899533e-7\n",
            "  Newton:\n",
            "    Iterações: 1\n",
            "    Tempo: 0.000229 s\n",
            "    Gradiente: 2.051026942305451e-14 \n",
            "\n",
            "Dimensão: 100, Caso: spread\n",
            "  Gradiente:\n",
            "    Iterações: 7225\n",
            "    Tempo: 0.078565 s\n",
            "    Gradiente: 9.982266048871296e-7\n",
            "  Newton:\n",
            "    Iterações: 1\n",
            "    Tempo: 0.000252 s\n",
            "    Gradiente: 2.6336295247372963e-12 \n",
            "\n",
            "Dimensão: 1000, Caso: equal\n",
            "  Gradiente:\n",
            "    Iterações: 1\n",
            "    Tempo: 0.00281 s\n",
            "    Gradiente: 1.9499192892063954e-14\n",
            "  Newton:\n",
            "    Iterações: 1\n",
            "    Tempo: 0.334478 s\n",
            "    Gradiente: 1.0606513635912954e-14 \n",
            "\n",
            "Dimensão: 1000, Caso: two\n",
            "  Gradiente:\n",
            "    Iterações: 17\n",
            "    Tempo: 0.026209 s\n",
            "    Gradiente: 4.224753452658551e-7\n",
            "  Newton:\n",
            "    Iterações: 1\n",
            "    Tempo: 0.050094 s\n",
            "    Gradiente: 1.212313944847135e-13 \n",
            "\n",
            "Dimensão: 1000, Caso: spread\n",
            "  Gradiente:\n",
            "    Iterações: 7085\n",
            "    Tempo: 9.047043 s\n",
            "    Gradiente: 9.996046466466697e-7\n",
            "  Newton:\n",
            "    Iterações: 1\n",
            "    Tempo: 0.035638 s\n",
            "    Gradiente: 1.9605642441201785e-11 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa-se que o método de Newton converge em apenas uma iteração para todos os casos testados, independentemente da dimensão da matriz ou da distribuição dos autovalores. Isso ocorre porque, para uma função quadrática, a direção de Newton aponta exatamente para o mínimo. O tempo de execução aumenta levemente com a dimensão devido à necessidade de resolver o sistema linear $H p = g$.\n",
        "\n",
        "Por outro lado, o desempenho do método do gradiente dependente do condicionamento da matriz $A$. Nos casos de autovalores iguais ou com apenas dois distintos, o número de iterações necessárias é relativamente baixo. Mas quando os autovalores estão muito diferentes, o método precisa de mais iterações para convergir. O tempo de processamento cresce à medida que a dimensão da matriz aumenta."
      ],
      "metadata": {
        "id": "zn1JgXKCjth3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Julia",
      "name": "julia"
    },
    "language_info": {
      "name": "julia"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}